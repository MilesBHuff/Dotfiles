#!/usr/bin/env -S sysctl -p
## This file contains settings related to memory, disk IO, and processes.
## Copyright Â© by Miles Bradley Huff from 2016-2024 per the LGPL3 (the Third Lesser GNU Public License)

## "Laptop mode", which allows HDD's to spin down
## Configuration is located at /etc/default/laptop-mode
# vm.laptop_mode=1

## Try to consolidate blocks of free memory
vm.compact_memory=1
## Can cause minor stalling if enabled, so leave disabled
vm.compact_unevictable_allowed=0

## 0 allows heuristics;  1 is ruthless.  Both have merits.
vm.memory_failure_early_kill=0
vm.oom_kill_allocating_task=0

## How to handle overcommitting memory
vm.overcommit_memory=0
## In mode #2 above, allow the system to overcommit up to 80% of system memory (and no more)
vm.overcommit_ratio=80
## These are set to the following sane values by default on any system with enough RAM:
# vm.admin_reserve_kbytes=8192
# vm.user_reserve_kbytes=131072

## Should be 0 for PCs
vm.zone_reclaim_mode=0

## Self-explanatory
vm.memory_failure_recovery=1
vm.legacy_va_layout=0

## ((200 - swappiness) / 2)=preference for swapping cache pages vs swapping anonymous pages
## Default is '60', but '66' is more balanced ('66' has a apx 1:2 ratio, whereas '60' only roughly approximates that).
## Advice to set swappiness to 0 is misguided;  don't do it.
vm.swappiness=66
## Very low values are bad, but the default isn't great for perceived performance.
## By analogy with swappiness's default, I'm setting this to '33'.  Default is 100.
vm.vfs_cache_pressure=33

## Default is `3`, but `4` is appropriate for systems running off NVMes and possessing high CPU core counts.
# vm.page-cluster=4

## Best type;  doesn't eat up any ports.
kernel.io_delay_type=2

## Default is 30 seconds, or `3000`.
# vm.dirty_expire_centisecs=3000
## The default value (500) has in the past caused problems with kworker CPU usage.
vm.dirty_writeback_centisecs=1000

## How much data in need of writing to disk can the system cache in RAM before writing?
## Default is 20% (normal, meaning "process") and 10% ("background", meaning "system").
##
## You can set either "bytes" OR "ratio" (of RAM) -- not both!
## Should ideally use bytes, because the value you want is dependent on the bandwidths of your drives, not the amount of memory you have.
## I would guess that ratio is the default because the optimal byte value is often higher than is reasonable for the given system's memory... or at least was at a time when RAM was much lower than it is now.
## Accordingly, I'd say either use the optimal byte value (and use it for both normal and background) if it is less than 20%/10%; else, use the default 20%/10% ratios.
##
## I would guess that these should optimally be set to the write speed of your block device.
## Of course, though, this varies widely by what's being written to the disk, and filesystem/RAID/etc have impacts on this as well.
##
## A reasonable upper bound is probably the maximum bandwidth supported by your slowest system drive.
## This is because storing dirty data past that limit seemingly necessarily results in blocking IO during the next sync.
## As an example of a value: SATAIII has a data transfer speed of 6 gigabits per second; so 750MBs would be the theoretical maximum byte value for a drive that can saturate that connection (SATA SSDs).
## HDDs are as a rule slower than SATAIII, so you would want to use a lower value for them.
## NVMes vary wildly in their capabilities, because the PCIe version and the number of lanes provided varies between computers.
##
## Since real-world performance is virtually always lower than theoretical maximums, maybe take 80% of whatever theoretical figure you find, and use that for this setting.
## The 80% is arbitrary, but inspired by the 80/20 Rule.
##
## After adjusting your value thusly, consider the maximum amount of time you are okay having an application be blocked waiting on I/O.
## It is said in the field of HCI that humans perceive delays of about 100 milliseconds as instantaneous;  let's go with this figure.
## Since our bandwidth value was per second, we need to adjust it to be per 100ms.
## That amounts to multiplying the bandwidth value by 100/1000, which is equivalent to dividing the bandwidth by 10.
##
## All together, the formula is `t/12.5`, simplified from `(t * 0.80) * (100/1000)` (where `t` stands for "hypothetical max throughput").
##
## Cheap microSD card:  10MB/s
# vm.dirty_bytes=838861
# vm.dirty_background_bytes=838861
## Quality microSD card over USB2:  40MB/s
# vm.dirty_bytes=3355443
# vm.dirty_background_bytes=3355443
## UHS-I microSD card:  95MB/s
# vm.dirty_bytes=7969178
# vm.dirty_background_bytes=7969178
## UHS-II microSD card:  275MB/s
# vm.dirty_bytes=23068672
# vm.dirty_background_bytes=23068672
## UHS-III microSD card:  450MB/s
# vm.dirty_bytes=37748736
# vm.dirty_background_bytes=37748736
##  5400 RRM SATAIII HDD:  120MB/s
# vm.dirty_bytes=10066330
# vm.dirty_background_bytes=10066330
##  7200 RRM SATAIII HDD:  150MB/s
# vm.dirty_bytes=12582912
# vm.dirty_background_bytes=12582912
## 10000 RRM SATAIII HDD:  225MB/s
# vm.dirty_bytes=18874368
# vm.dirty_background_bytes=18874368
## SATAIII SSD:  750MB/s
# vm.dirty_bytes=62914560
# vm.dirty_background_bytes=62914560
## NVMe v1.3 drive running on PCIe 3.0 x4:  3.94 GB/s
# vm.dirty_bytes=338443423
# vm.dirty_background_bytes=338443423
## NVMe v1.4 drive running on PCIe 4.0 x4:  7.88 GB/s
# vm.dirty_bytes=676886846
# vm.dirty_background_bytes=676886846
## NVMe v2.0 drive running on PCIe 5.0 x4: 15.75 GB/s
# vm.dirty_bytes=1352914698
# vm.dirty_background_bytes=1352914698
