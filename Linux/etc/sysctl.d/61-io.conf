#!/usr/bin/env -S sysctl -p
## This file contains settings related to memory, disk IO, and processes.
## Copyright © by Miles Bradley Huff from 2016-2026 per the LGPL3 (the Third Lesser GNU Public License)

## Allows you to use ACPI scripts that avoid spinning up your HDD while you are on battery. The downside? You risk losing up to 10 minutes of data in a crash or sudden loss of power.
## Configuration is located at /etc/default/laptop-mode
vm.laptop_mode=0

## Try to consolidate blocks of free memory.
## This is a oneshot command, not a configuration; accordingly, it is commented.
# vm.compact_memory=1
## Can cause minor stalling if enabled, so leave disabled. Default, oddly-enough, is enabled.
vm.compact_unevictable_allowed=0

## 0 allows heuristics; 1 is ruthless. Both have merits.
vm.memory_failure_early_kill=0
vm.oom_kill_allocating_task=0

## How to handle overcommitting memory
vm.overcommit_memory=0
## In mode #2 above, allow the system to overcommit up to 80% of system memory (and no more)
vm.overcommit_ratio=80
## These are set to the following sane values by default on any system with enough RAM:
# vm.admin_reserve_kbytes=8192
# vm.user_reserve_kbytes=131072

## Should be 0 unless you have a system that has to worry about NUMA. (You almost certainly don't.)
vm.zone_reclaim_mode=0

## Try to recover from unrecoverable ECC errors
vm.memory_failure_recovery=1
## This determines whether, when there is an uncorrectable ECC error, to kill affected applications immediately or to kill them only when they try to interact with the corrupted page(s)
## Set to 1 for maximum safety guarantees, set to 0 if you're already in a sloppy situation (like a desktop on consumer storage media and without ECC memory).
vm.memory_failure_early_kill=1

## ((200 - swappiness) / 2) = preference for swapping anonymous pages vs dropping cache pages
## Essentially, this value is a 0–200 score of how detrimental swapping application memory is vs how hard reading from disk is. Lower values tell the kernel that swapping is costly; higher that disk reads are.
## Advice to set swappiness to 0 is misguided; don't do it.
## The default is '60' (a rough approximation of 1/3). I prefer to use a closer approximation of 1/3 as my personal default: '66'.
## If your swap device is zram, you should set this higher, potentially much higher; take 100 as a starting place. (Why? Because swapping to compressed RAM is extremely fast compared to reading from disk.)
vm.swappiness=66
## If lower than denom, the kernel prefers metadata (inodes, dentries); if higher, data.
## I want the kernel to prefer to hold on to metadata, since responsiveness is more-impacted by it than by data.
## For a 2:1 metadata:data preference, we need to set 50:100 _:denom.
vm.vfs_cache_pressure=50
## This is what the above value is relative to. Defaults to its minimum, 100.
vm.vfs_cache_pressure_denom=100

## How many swap pages to read in one go, expressed in exponents of 2. Default is `3`. Reduce to improve swapping latency, increase to improve swapping throughput. This is effectively a form of readahead when nonzero.
# vm.page-cluster=3

## How to go about adding a tiny delay to avoid issues with some legacy hardware. A value of `3` disables, `0` and `1` use up a specific kind of rarely-used port on the computer (which may not even add the requisite delay), and `2` perfectly simulates a 2 microsecond delay without using any ports.
kernel.io_delay_type=2

## Legacy behavior
vm.legacy_va_layout=0

## The default is 5s, which matches ZFS's `zfs_txg_timeout`. I don't see any major reason to change on modern systems, but it's worth noting that I have seen excessive kworker CPU usage from this default in the past. If you see that, try increasing it to `1000`.
vm.dirty_writeback_centisecs=500
## The default is 30s, but that's kind of long; I'd rather use something shorter. ZFS has an effective 15s limit, as it has three TXGs of 5s each; so I will use 15s as a sane value.
## If you increase `dirty_writeback_centisecs`, make sure you adjust this to stay about 3x as much.
vm.dirty_expire_centisecs=1500

## How much data in need of writing to disk can the system cache in RAM before writing?
## Default is 10% (for start of background writeback) and 20% (for start of foreground writeback), with these percents relative to total system memory.
## This is a 1:2 ratio. You can select a different ratio, but background must always be less than foreground.
##
## You can set either "bytes" OR "ratio" (of RAM) -- not both!
## Ideally, you should use "bytes", because the value you want is dependent on the typical sustained write throughput of your slowest core block device, not the amount of memory you have.
##   We might as well size dirty data to match the 5s window defined by `dirty_writeback_centisecs`.
##     Accordingly: Benchmark the sustained write performance (in MB/s) of your slowest core block device, then multiply that by 5 and convert to bytes: this is your optimal `dirty`. Divide that by `2`; this is your optimal `dirty_background`.
## I would guess that "ratio" is the default because when systems had less RAM, the optimal absolute byte value was often higher than was reasonable for a given system's memory.
##   Nowadays, it's not out-of-the-question for a high-performance computer to have 128GB of RAM; and for such a system, "ratio" results in insanities.
##     For insance: with 128GB of RAM, Linux allows up to nearly 26GB of dirty data before forcing foreground writeback. That's a ton of data loss if the system crashes or loses power, it's not a great situation for system responsiveness, and it increases the likelihood of you experiencing memory pressure.
##       Even normal computers can be affected: with 16GB of RAM, Linux by default allows up to 3.2GB to be dirty, but the above [tuned] formula would yield only [roughly] 1.25GB with a consumer SATA SSD. (The same system with an NVMe, though, should stick with "ratio".)
## Accordingly, I'd say use "bytes" if your optimal size is less than 10%/20%; else, use "ratio" to avoid memory pressure.
##
## Consumer HDD:
# vm.dirty_bytes=625000000
# vm.dirty_background_bytes=312500000
## Enterprise SATA HDD / Consumer SATA SSD:
vm.dirty_bytes=1250000000
vm.dirty_background_bytes=625000000
## Enterprise SATA SSD:
# vm.dirty_bytes=2500000000
# vm.dirty_background_bytes=1250000000
## NVMe: (This varies a LOT by your specific system; please benchmark!)
# vm.dirty_bytes=20000000000
# vm.dirty_background_bytes=10000000000

## Whether to enable deduplication of anonymous pages. Enable if hosting long-lived VMs. Don't enable if you don't control all those VMs.
kernel.mm.ksm.run=0
## Scan 32GB of RAM every 23hrs
kernel.mm.ksm.pages_to_scan=100
kernel.mm.ksm.sleep_millisces=1000
