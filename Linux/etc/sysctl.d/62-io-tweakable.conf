#!/usr/bin/env -S sysctl -p
## This file contains settings related to memory, disk IO, and processes.
## Copyright © by Miles Bradley Huff from 2016-2026 per the LGPL3 (the Third Lesser GNU Public License)

################################################################################
## LIKELY TO NEED PER-SYSTEM TWEAKING
################################################################################

## Should be 0 unless you have a system that has to worry about NUMA. (You almost certainly don't.)
vm.zone_reclaim_mode=0

## ((200 - swappiness) / 2) = preference for swapping anonymous pages vs dropping cache pages
## Essentially, this value is a 0–200 score of how detrimental swapping application memory is vs how hard reading from disk is. Lower values tell the kernel that swapping is costly; higher that disk reads are.
## Advice to set swappiness to 0 is misguided; don't do it.
## The default is '60' (a rough approximation of 1/3). I prefer to use a closer approximation of 1/3 as my personal default: '66'.
## If your swap device is zram, you should set this higher, potentially much higher; take 100 as a starting place. (Why? Because swapping to compressed RAM is extremely fast compared to reading from disk.)
vm.swappiness=66
## If lower than denom, the kernel prefers metadata (inodes, dentries); if higher, data.
## I want the kernel to prefer to hold on to metadata, since responsiveness is more-impacted by it than by data.
## For a 2:1 metadata:data preference, we need to set 50:100 _:denom.
vm.vfs_cache_pressure=50
## This is what the above value is relative to. Defaults to its minimum, 100.
vm.vfs_cache_pressure_denom=100

## Whether to enable deduplication of anonymous pages. Enable if hosting long-lived VMs. Don't enable if you don't control all those VMs, as a determined attacker may be able to determine which pages are deduped.
kernel.mm.ksm.run=0
## Scan 32GB of RAM every 23hrs
kernel.mm.ksm.pages_to_scan=100
kernel.mm.ksm.sleep_millisecs=1000

## The default is 5s, which matches ZFS's `zfs_txg_timeout`. I don't see any major reason to change on modern systems, but it's worth noting that I have seen excessive kworker CPU usage from this default in the past. If you see that, try increasing it to `1000`.
vm.dirty_writeback_centisecs=500
## The default is 30s, but that's kind of long; I'd rather use something shorter. ZFS has an effective 15s limit, as it has three TXGs of 5s each; so I will use 15s as a sane value.
## If you increase `dirty_writeback_centisecs`, make sure you adjust this to stay about 3x as much.
vm.dirty_expire_centisecs=1500

## How much data in need of writing to disk can the system cache in RAM before writing?
## Default is 10% (for start of background writeback) and 20% (for start of foreground writeback), with these percents relative to total system memory.
## This is a 1:2 ratio. You can select a different ratio, but background must always be less than foreground.
##
## You can set either "bytes" OR "ratio" (of RAM) -- not both!
## Ideally, you should use "bytes", because the value you want is dependent on the typical sustained write throughput of your slowest core block device, not the amount of memory you have.
##   We might as well size dirty data to match the 5s window defined by `dirty_writeback_centisecs`.
##     Accordingly: Benchmark the sustained write performance (in MB/s) of your slowest core block device, then multiply that by 5 and convert to bytes: this is your optimal `dirty`. Divide that by `2`; this is your optimal `dirty_background`.
## I would guess that "ratio" is the default because when systems had less RAM, the optimal absolute byte value was often higher than was reasonable for a given system's memory.
##   Nowadays, it's not out-of-the-question for a high-performance computer to have 128GB of RAM; and for such a system, "ratio" results in insanities.
##     For insance: with 128GB of RAM, Linux allows up to nearly 26GB of dirty data before forcing foreground writeback. That's a ton of data loss if the system crashes or loses power, it's not a great situation for system responsiveness, and it increases the likelihood of you experiencing memory pressure.
##       Even normal computers can be affected: with 16GB of RAM, Linux by default allows up to 3.2GB to be dirty, but the above [tuned] formula would yield only [roughly] 1.25GB with a consumer SATA SSD. (The same system with an NVMe, though, should stick with "ratio".)
## Accordingly, I'd say use "bytes" if your optimal size is less than 10%/20%; else, use "ratio" to avoid memory pressure.
##
## Consumer HDD:
# vm.dirty_bytes=625000000
# vm.dirty_background_bytes=312500000
## Enterprise SATA HDD / Consumer SATA SSD:
vm.dirty_bytes=1250000000
vm.dirty_background_bytes=625000000
## Enterprise SATA SSD:
# vm.dirty_bytes=2500000000
# vm.dirty_background_bytes=1250000000
## NVMe: (This varies a LOT by your specific system; please benchmark!)
# vm.dirty_bytes=20000000000
# vm.dirty_background_bytes=10000000000
